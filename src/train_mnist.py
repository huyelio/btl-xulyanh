"""
Script hu·∫•n luy·ªán CNN model cho MNIST
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import os
from preprocessing import ImagePreprocessor


def create_mnist_model(input_shape=(28, 28, 1), num_classes=10):
    """
    T·∫°o CNN model cho MNIST
    
    Architecture:
    - Conv2D (32 filters) + MaxPooling
    - Conv2D (64 filters) + MaxPooling
    - Conv2D (64 filters) + MaxPooling
    - Flatten + Dense(128) + Dense(10)
    """
    model = keras.Sequential([
        # Input layer
        layers.Input(shape=input_shape),
        
        # Convolutional Block 1
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Dropout(0.25),
        
        # Convolutional Block 2
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Dropout(0.25),
        
        # Convolutional Block 3
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.Dropout(0.25),
        
        # Fully Connected Layers
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model


def load_and_preprocess_mnist():
    """T·∫£i v√† ti·ªÅn x·ª≠ l√Ω MNIST dataset"""
    print("ƒêang t·∫£i MNIST dataset...")
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    
    print(f"Train shape: {x_train.shape}")
    print(f"Test shape: {x_test.shape}")
    
    # Reshape v√† chu·∫©n h√≥a
    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0
    
    # Convert labels to categorical
    y_train = keras.utils.to_categorical(y_train, 10)
    y_test = keras.utils.to_categorical(y_test, 10)
    
    print(f"‚úì ƒê√£ chu·∫©n b·ªã d·ªØ li·ªáu")
    print(f"  - X_train: {x_train.shape}, Y_train: {y_train.shape}")
    print(f"  - X_test: {x_test.shape}, Y_test: {y_test.shape}")
    
    return (x_train, y_train), (x_test, y_test)


def plot_training_history(history, save_path='models/mnist_history.png'):
    """V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Accuracy
    ax1.plot(history.history['accuracy'], label='Train Accuracy')
    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')
    ax1.set_title('Model Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)
    
    # Loss
    ax2.plot(history.history['loss'], label='Train Loss')
    ax2.plot(history.history['val_loss'], label='Val Loss')
    ax2.set_title('Model Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì training history t·∫°i {save_path}")


def demonstrate_preprocessing():
    """Demo pipeline ti·ªÅn x·ª≠ l√Ω v·ªõi m·ªôt v√†i ·∫£nh MNIST"""
    print("\n" + "="*60)
    print("DEMO: Pipeline ti·ªÅn x·ª≠ l√Ω ·∫£nh MNIST")
    print("="*60)
    
    # Load m·ªôt v√†i ·∫£nh m·∫´u
    (x_train, y_train), _ = keras.datasets.mnist.load_data()
    
    # T·∫°o preprocessor v·ªõi save_progress=True
    preprocessor = ImagePreprocessor(save_progress=True, output_dir="example_progress")
    
    # L·∫•y m·ªôt ·∫£nh m·∫´u (ch·ªØ s·ªë 7)
    sample_indices = [0, 100, 200]
    
    for idx in sample_indices:
        sample_img = x_train[idx]
        sample_label = y_train[idx]
        
        print(f"\nX·ª≠ l√Ω ·∫£nh m·∫´u {idx} (nh√£n: {sample_label})...")
        
        # Ch·∫°y full pipeline
        processed = preprocessor.full_pipeline(sample_img, for_mnist=True)
        
        # L∆∞u progress images
        preprocessor.save_progress_images(prefix=f"mnist_sample_{idx}")
        
        print(f"‚úì ƒê√£ x·ª≠ l√Ω v√† l∆∞u {len(preprocessor.get_progress_images())} b∆∞·ªõc")


def train_mnist_model(epochs=30, batch_size=128, save_dir='models', use_augmentation=True):
    """
    Hu·∫•n luy·ªán MNIST model v·ªõi Data Augmentation
    
    Args:
        epochs: S·ªë epoch (m·∫∑c ƒë·ªãnh 30 v·ªõi augmentation)
        batch_size: Batch size
        save_dir: Th∆∞ m·ª•c l∆∞u model
        use_augmentation: S·ª≠ d·ª•ng Data Augmentation ƒë·ªÉ gi·∫£i quy·∫øt Domain Gap
    """
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs(save_dir, exist_ok=True)
    
    print("\n" + "="*60)
    print("HU·∫§N LUY·ªÜN CNN MODEL CHO MNIST")
    print("="*60)
    
    # Load data
    (x_train, y_train), (x_test, y_test) = load_and_preprocess_mnist()
    
    # üöÄ DATA AUGMENTATION - Gi·∫£i ph√°p cho Domain Gap!
    if use_augmentation:
        print("\nüé® T·∫°o Data Augmentation Generator...")
        print("   ‚Üí Gi√∫p model quen v·ªõi ·∫£nh b·ªã l·ªách, xoay, zoom...")
        from tensorflow.keras.preprocessing.image import ImageDataGenerator
        
        datagen = ImageDataGenerator(
            rotation_range=15,       # Ng·∫´u nhi√™n xoay +/- 15 ƒë·ªô
            width_shift_range=0.15,  # Ng·∫´u nhi√™n d·ªãch ngang 15%
            height_shift_range=0.15, # Ng·∫´u nhi√™n d·ªãch d·ªçc 15%
            zoom_range=0.15,         # Ng·∫´u nhi√™n ph√≥ng to/thu nh·ªè 15%
            shear_range=0.1,         # Ng·∫´u nhi√™n l√†m m√©o ·∫£nh
            fill_mode='constant',    # Fill ph·∫ßn tr·ªëng b·∫±ng 0 (m√†u ƒëen)
            cval=0
        )
        
        # Fit datagen v√†o training data
        datagen.fit(x_train)
        print("‚úì Data Augmentation ready!")
    
    # T·∫°o model
    print("\nT·∫°o model...")
    model = create_mnist_model()
    
    # Compile model
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # In t√≥m t·∫Øt model
    model.summary()
    
    # Callbacks
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            os.path.join(save_dir, 'mnist_model_best.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # Train model
    print(f"\nB·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ({epochs} epochs, batch_size={batch_size})...")
    if use_augmentation:
        print("   ‚Üí S·ª≠ d·ª•ng Data Augmentation - model s·∫Ω khoan dung h∆°n v·ªõi ·∫£nh th·ª±c t·∫ø!")
        history = model.fit(
            datagen.flow(x_train, y_train, batch_size=batch_size),
            epochs=epochs,
            validation_data=(x_test, y_test),
            steps_per_epoch=len(x_train) // batch_size,
            callbacks=callbacks,
            verbose=1
        )
    else:
        history = model.fit(
            x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
    
    # Evaluate
    print("\nƒê√°nh gi√° model tr√™n test set...")
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
    print(f"‚úì Test accuracy: {test_acc:.4f}")
    print(f"‚úì Test loss: {test_loss:.4f}")
    
    # Save final model
    model_path = os.path.join(save_dir, 'mnist_model.h5')
    model.save(model_path)
    print(f"\n‚úì ƒê√£ l∆∞u model t·∫°i {model_path}")
    
    # Plot history
    plot_training_history(history, save_path=os.path.join(save_dir, 'mnist_history.png'))
    
    # Test v·ªõi m·ªôt v√†i predictions
    print("\n" + "="*60)
    print("TEST: D·ª± ƒëo√°n m·ªôt v√†i ·∫£nh m·∫´u")
    print("="*60)
    
    sample_indices = [0, 100, 200, 500, 1000]
    predictions = model.predict(x_test[sample_indices], verbose=0)
    
    for i, idx in enumerate(sample_indices):
        true_label = np.argmax(y_test[idx])
        pred_label = np.argmax(predictions[i])
        confidence = predictions[i][pred_label]
        
        status = "‚úì" if true_label == pred_label else "‚úó"
        print(f"{status} ·∫¢nh {idx}: True={true_label}, Pred={pred_label}, Confidence={confidence:.4f}")
    
    return model, history


if __name__ == "__main__":
    # Demo preprocessing pipeline
    demonstrate_preprocessing()
    
    # Train model v·ªõi Data Augmentation
    model, history = train_mnist_model(
        epochs=30,  # TƒÉng epochs v√¨ model ph·∫£i h·ªçc b√†i to√°n kh√≥ h∆°n
        batch_size=128,
        save_dir='models',
        use_augmentation=True  # B·∫≠t Data Augmentation ƒë·ªÉ gi·∫£i quy·∫øt Domain Gap
    )
    
    print("\n" + "="*60)
    print("‚úì HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN MNIST MODEL")
    print("‚úì Model ƒë√£ ƒë∆∞·ª£c train v·ªõi Data Augmentation!")
    print("‚úì B√¢y gi·ªù model s·∫Ω ch√≠nh x√°c h∆°n v·ªõi ·∫£nh vi·∫øt tay th·ª±c t·∫ø!")
    print("="*60)

